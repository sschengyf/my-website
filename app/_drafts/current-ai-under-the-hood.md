---
layout: post
title: Why hasn't AI replaced us
caption: '为何AI还没有取代我们'
author: Ivan Cheng
categories: AI Technology
tags: AI
---

毫无疑问，这两年最热门的话题无疑是 AI，我记得大概在 2020 年前后还问过一个搞 AI 的朋友，什么时候 AI 会替代程序员的工作，那时他说很难。没想到 2022 年 OpenAI 的 ChatGPT 横空出世，举世震惊，好像全世界都没有做好心理准备的情况下，AI 的拐点就出现在面前，一夜之间犹如科幻照进现实。震惊之余，当时讨论最多的话题有哪些工作最容易被 AI 取代，或者 AI 什么时候会取代人，要么是做哪些工作最不容易被 AI 取代，史称第一次程序员危机。三年过去了，时至今日，虽然让人担心的巨大裁员还没发生，但 AI 何时取代人的工作的话题依然在热烈讨论，恰逢这两年确有不少公司裁员，多则几千人少则几十上百人，裁员的主要原因不见得是 AI 的出现导致的。这里有一个大的背景不应该被忽视，大瘟疫时期的低利息环境让许多公司轻松融资，不少公司疯狂投资和扩编人员，造成人员超编，公司运营效率降低，疫情结束之后，为降低通胀，各国央行执行升息政策，经济环境恶化，公司利润下滑，公司希望通过或多或少的裁员来降低运营成本来提高利润。这时 AI 的适时出现，和公司的裁员需求碰上， AI 是个体面的裁员借口还是真的完全替代了被裁掉员工的工作，以至于还有公司推出 AI 产品号称能代替人工。

现在 AI 的发展处于方兴未艾的阶段，时刻都有新的名词和新概念被发明出来，AI agents，MCP，RAG, LangChain 等等，这些新概念本质上关注如何有效应用 AI 来解决现有的问题。当然应用对产业发展至关重要，如果不能应用，产生不了价值，这个产业便不能可持续的吸引投资和人才。但这些新的概念并非 AI 取代人的关键，核心还是目前 AI 的原理。所以我想看看当前 AI 原理来探讨目前 AI 对取代人类的可能。

## AI 处于什么阶段

理论上，AI 的发展会经历三个阶段，弱 AI（Weak AI, Narrow AI）简称 ANI，强 AI（Strong AI）也叫 AGI，超级 AI（Artificial Super Intelligence），简称 ASI。

即便我们感觉现在的 AI 已经很强大了，但 AI 还尚处在弱 AI 的阶段。因为当前的 AI 模型都是训练出来解决某些特定范围内的问题，也可以说 AI 模型的训练的数据是巨量的有明确答案的数据，一旦尝试靠它来解决超出它训练范围的问题时就显得不太灵光了。有时对我们的提问，AI 的回答就似胡编乱造(hallucination)，即便你告诉它如果你不知道请说不知道，可是 AI 从不承认自己的无知，其实并非 AI 不愿承认，而是它不清楚自己找到的答案并不正确，这难免让人怀疑 AI 是否真的有推理能力，或许什么是推理。也许 AI 给我们造成的压迫感，更多还是由于 AI 模型在知识储量上绝对胜过任何人类个体，毕竟它们学遍了整个互联网上公开的知识，但知识量的碾压并不能代表能力上的碾压。

更进一步，来到强 AI （AGI）阶段，或者叫通用 AI，这类 AI 能够处理问题的能力没有范围限制，并且不断的自我学习和自我进化，到这个时候，AI 的能力无限接近接近人类。

最后，来到超级 AI（ASI）阶段，这时的 AI 超越人类所有的优点，拥有自我意识，毫无疑问最大的风险在于一旦它真的出现，人类有能力去掌控它吗？

## 当前的 AI

人工智能（AI）的概念在电子计算机发明不久的 1950 年代就被 John McCarthy 提出了，他深信计算机除了计算，应该还能走得更远，计算机能学习并具备所有智慧的功能（“Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.”），通俗地说，就是让计算机拥有与人一样学习和思考的能力。经过几代人几十年的探索，当前实现 AI 的路径大致是从机器学习到神经网络到深度学习到现在的大语言模型，像剥洋葱一层一层被探索出来，这条路径能否带领我们走向 AGI 和 ASI，到时候就知道了。

![AI path](../assets/imgs/ai-path.jpg)

机器学习（machine learning），简单来说就是让机器使用算法去学习我们提供的训练数据从而寻找数据里的规律（pattern），然后让机器用找到的规律来识别新的数据并做出预测和判断。这样做相较于一般的基于规则编写的计算机程序，虽然牺牲了一些准确性，但得到了更好的灵活性，在现实世界里，很多场景是不可能完全靠人力来提前预设所有变量并制定相应规则，图像识别是一个经典的例子，也就是说机器学习为我们提供了一个当前主流的基于规则系统（Rule Based System）之外的选择。

![Neural Network](../assets/imgs/neural-network.png)

机器学习有不同的实现方法，神经网络（neural networks）是其中之一，作为一种模型框架，它的诞生受到人脑生物神经元（neuron）的启发。神经元的概念并不复杂，每个神经元带有三个关键的值。

1. 激活值（activation），这是一个从 0 到 1 的值，每一个神经元的激活值都由上一层所有神经元的激活值作为输入变量决定。
2. 权重（weight）代表当前层次某个神经元与上一层每个神经元之间联系的强弱，上一层每个神经元与当前这个神经元有一个单独的权重。假如上一层有 20 个神经元，那么在当前层每个神经元与上一层都有 20 个权重。
3. 偏置项（bias），相当于激活神经元各自需要的门槛值。

一个神经网络由多层（大于 3 层）的神经元构成，第一层接收数据的输入，然后各层之间从上往下传递数值，在传递过程中，每个神经元对输入的数值计算之后得到一个新的数值又传递给下一层的神经元，直到最后一层神经元，最后一层里激活值最高的那个神经元就是神经网络给出的结果。计算过程是上一层每个神经元的激活值乘以相对应的下一层神经元权重，然后求和，在加上偏置项，就得到下一层神经元的激活值，如此层层递进计算。这个过程的计算方程如下

$$
y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
$$

所谓的"学习"，实质上在给神经网络寻找一套正确的权重和偏置项，也是我们常听说的参数，把这些学习获得的参数保存下来就是模型。换个角度看，每个神经元是一个函数，这个函数自带两个常量（权重和偏置项），它接收一个变量作为输入然后输出一个结果（激活值），把这个结果作为输入参数投入下一个神经元函数，层层递进直到结束。一般来说，神经网络的神经元越多效果越好，当然参数自然也越多。一个普通的识别图片上 0 到 9 数字的模型参数规模都能上万，现在一个大语言模型的参数规模普遍达到百亿到万亿级别，因此各家厂商在宣传自己模型性能时都会强调参数的大小。

> Neurons that fire together wire together 一起激活的神经元，会彼此强化连接
>
> From Hebbian theory

想要找到这套正确的参数，通过一种叫做反向传播（Backpropagation）的操作来实现，如果我们把输入变量到神经元得到一个激活值，然后把激活值作为下一个神经元的变量输入得到下一个激活值，依次递进直到最终结果的过程称为正向传播，那么反向传播就是把顺序反过来，从已知结果来层层倒推反向寻找每个神经元正确的参数（权重，偏置项），这个过程用微积分（Calculus）方程来计算。

这种通过反向传播训练神经网络原理在 1970 年代便存在了，在 1980 年代在 David Rumelhart, Geoffrey Hinton, Ronald Williams 的推动下被主流接受。即使数学上的理论有了，可是在实践过程中研究者们遇到很多挫折，受限于早期神经网络架构（CNN，RNN）的设计，只能使用串行计算来进行模型训练，如此一来训练速度的上限很低，在数据量小的时候，训练耗时还能接受，一旦投喂大量的数据来做训练，训练耗时会达到无法接受的程度，同时因为不能进行大规模训练，也就没有机会诞生大模型。另外不能理解全局上下文也是早期架构的一个重要缺陷，你是否记得早期的苹果 Siri 几乎只能完成一句话的聊天，完全不能进行多个来回的对话。再加上早期架构的一些其他缺陷使得人工智能领域一直没能推出让人满意的产品。

模型不能直接理解人类的文字，要先把文字（word）切分成令牌（token），也就是数字 ID，然后用令牌 ID 去查嵌入矩阵（embedding matrix）中找到对应的向量（vector），如此一来文字变成了模型能理解的对象，接下来，这些向量会依次经过模型内部的很多矩阵（matrix）运算，也就是神经网络的各层。每一层的矩阵都是在训练过程中学习出来的，用来不断把向量转换成更高层次的表示。向量经过层层矩阵的传递之后，模型会输出一个对整个词表（vocabulary）所有 token 的分数向量（logits）。经过 softmax 转换成概率后，取概率最高的 token 作为输出。向量，也叫矢量，简单说是有方向的数值，还记得我们在初中物理学习受力分析时肯定接触过向量的概念，力的大小仅是一个数值，受力分析最重要的是考虑力的方向，这就是说，力的数值需要力的方向来赋予意义，一个没有方向的纯数值是没有意义的。只不过当时学习受力分析的向量只有两个维度，而机器学习里的向量有上万个维度，但原理并无二致，都是通过方向为数值赋予意义。

在此期间，尽管没有诞生出让人满意的 AI 产品，但重要的是机器学习界摸索出了训练和使用模型的基本流程，以及至今我们耳熟能详的概念。

临门一脚出现在 2017 年，大名鼎鼎的 Transformer 架构随着那篇著名的论文 [Attention Is All You Need](https://arxiv.org/abs/1706.03762) 的出现，改变了 AI 界乃至世界的发展轨迹。Transformer 架构带来的自我注意力（self-attention）机制和并行计算是诞生大模型和使得 AI 成为受欢迎产品的关键发明。

- q
- k
- v

## 缺少价值观

## 不会提问

在我们使用 AI 的过程中，AI 会主动提问吗，如果是和一个真人对话，真人面对不了解的问题，他们会通过提出一个又一个有关的问题来一步步接近答案，对人来说，提问是一个很重要的能力。

知道的东西越多，问题越多。

## 被取代的

- 搜索引擎
- 知识问答网站

## 科技以人为本

我发觉一个有意思的现象，几乎每次有关 AI 的讨论，最后话题又落到 AI 是否超越了人，是否能取代人，有的人很悲观，有的人很崇拜，也有的人不以为意。

媒体渲染，陶泽轩

![Connecting People](../assets/imgs/nokia-slogan.jpg)

## 参考

- [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk)
- [The 7 Types of AI - And Why We Talk (Mostly) About 3 of Them](https://www.youtube.com/watch?v=XFZ-rQ8eeR8)
- [What is machine learning?](https://www.ibm.com/think/topics/machine-learning#7281535)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)
